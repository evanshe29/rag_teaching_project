
import json
import faiss
from sentence_transformers import SentenceTransformer
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

# è·¯å¾„é…ç½®
# main.py ä¸­åŠ å…¥ï¼ˆæ”¾åœ¨æ–‡ä»¶é¡¶éƒ¨ï¼‰
import os
CURRENT_DIR = os.path.dirname(__file__)
INDEX_DIR = os.path.join(CURRENT_DIR, "..", "index", "faiss_index")

EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# åŠ è½½ FAISS ç´¢å¼•å’Œç»“æ„åŒ–å—
def load_data():
    index_path = os.path.join(INDEX_DIR, "doc.index")
    chunks_path = os.path.join(INDEX_DIR, "chunks.json")

    if not os.path.exists(index_path):
        raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
    if not os.path.exists(chunks_path):
        raise FileNotFoundError(f"æ–‡æœ¬å—æ–‡ä»¶ä¸å­˜åœ¨: {chunks_path}")

    print("ğŸ” æ­£åœ¨åŠ è½½ç´¢å¼•å’Œæ•™å­¦å†…å®¹...")
    idx = faiss.read_index(index_path)
    with open(chunks_path, "r", encoding="utf-8") as f:
        chunks = json.load(f)
    return idx, chunks

# æ£€ç´¢ç›¸ä¼¼å†…å®¹
def retrieve(question, idx, chunks, model, topk=5):
    emb = model.encode([question])
    D, I = idx.search(emb, topk)
    return [chunks[i] for i in I[0]]

# GPT å›ç­”
def answer_with_llm(question, context):
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.2)
    prompt = PromptTemplate.from_template(
        "æ ¹æ®ä»¥ä¸‹æ•™å­¦å†…å®¹å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š"
    )
    # â˜… æå–çº¯æ–‡æœ¬ç”¨äº Prompt æ„é€ 
    raw_texts = [chunk["text"] for chunk in context]
    full_prompt = prompt.format(context="\n".join(raw_texts), question=question)
    response = llm.invoke(full_prompt)
    return response.content.strip()

# ä¸»ç¨‹åº
if __name__ == "__main__":
    idx, chunks = load_data()
    embed_model = SentenceTransformer(EMBED_MODEL)
    question = input("è¯·è¾“å…¥é—®é¢˜ï¼š")
    context = retrieve(question, idx, chunks, embed_model)

    print("\n--- æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ ---")
    for i, c in enumerate(context):
        print(f"[{i + 1}] ({c['source']} ç¬¬ {c['page']} é¡µ, ä½ç½® {c['order']}): {c['text']}")

    print("\n--- GPT å›ç­” ---")
    print(answer_with_llm(question, context))
